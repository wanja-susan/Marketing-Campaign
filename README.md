# Marketing-Campaign Analysis

![20943871-marketing-scaled-removebg-preview](https://github.com/user-attachments/assets/8ffaf741-e319-4805-a4df-68724b67d884)

# **Project Overview**
Marketing Campaign Analysis is a data-driven Power BI dashboard project developed to evaluate the impact of a targeted campaign promoting QuickSave, a key feature in the Ziwatech fintech app. QuickSave empowers users, students, gig workers, small business owners, and everyday savers, to build consistent savings habits through automated micro-savings, goal-based plans, and behavioral nudges.

The goal of this campaign was to:

- Increase awareness and adoption of the QuickSave feature

- Boost engagement through email, app, web, and social campaigns

- Drive improvements in saving behavior, user retention, and Return on Investment(ROI)

The campaign was structured in three phases; Before, During, and After, with each phase offering a unique lens into user activity, channel performance, and campaign effectiveness. This analysis helps the Ziwatech team understand what worked, what didn’t, and how to continuously improve future outreach efforts.

Ultimately, this project supports Ziwatech’s mission: to change the narrative around saving, proving that you don’t need to “save big” to get started. With QuickSave, every small step counts.

<details>
<summary>Table of Contents</summary>

- [Project Overview](#project-overview)  
- [Data Sources](#data-sources)  
- [Methodology and Analysis Phases](#methodology-and-analysis-phases)  
  - [Before the Campaign](#before-the-campaign)  
  - [During the Campaign](#during-the-campaign)  
  - [After the Campaign](#after-the-campaign)  
- [Tools and Techniques](#tools-and-techniques)  
- [How to Use This Project](#how-to-use-this-project)  
- [Challenges](#challenges)  
- [Recommendations](#recommendations)  
- [Conclusion](#conclusion)  
- [Author & License](#author--license)

</details>

# **Data Sources**

This project is based on simulated campaign data collected during the QuickSave promotional campaign conducted by Ziwatech. The data was modeled to reflect a real-world, multi-channel campaign across mobile, web, email, and social platforms.

Data Types Used:

**User & Engagement Data**

- Total users, platform usage (web, app, social)

- Device types (desktop, mobile)

- Bounce rate and session duration

- Email subscription rate and wishlist usage

- Demographic segments (age group, gender, location)

**Marketing Campaign Metrics**

- Ad impressions and clicks

- Click-through rate (CTR), cost-per-click (CPC)

- Channel-level performance (App, Email, Social, Web)

- Daily campaign activities: email opens, spend, clicks

**Sales & Outcome Metrics**

- Conversion rate and acquisition cost

- Revenue and average order value

- Return on Investment (ROI) over time

- Retention metrics by channel and region

Assumptions and Constraints:

- Timeframe: The campaign ran from October 2024 to January 2025, segmented into three distinct phases for analysis.

- All figures (e.g., clicks, spend, revenue) are illustrative and anonymized for confidentiality.

- User segmentation (e.g., Loyal, Infrequent, Deal Seekers) was predefined using behavioral scoring.

- Revenue data reflects savings behavior (e.g., goal completions, deposit activity) rather than traditional e-commerce transactions.

- Engagement is tracked per channel and region (6 U.S. states), assuming consistent campaign delivery and tracking infrastructure.

This dataset provides a strong foundation for evaluating campaign impact and making informed strategic decisions for future marketing initiatives.

# **Methodology and Analysis Phases**

The analysis follows a structured approach across three distinct phases; Before, During, and After the campaign. Each phase serves a specific purpose in evaluating the effectiveness and ROI of the QuickSave promotional efforts.

### Before the Campaign
![image](https://github.com/user-attachments/assets/bb4e5144-99bd-4919-b5f4-ec213a8b7891)


https://github.com/user-attachments/assets/a3f8fb9c-33f8-4c7e-bd18-80d51bf2986f


The pre-campaign analysis focused on establishing performance baselines and setting realistic campaign targets. This included:

- Reviewing historical data on user activity, channel engagement, and goal conversion rates.

- Segmenting users by behavior (Loyal, Infrequent, Deal Seekers) and region (6 U.S. states).

- Identifying underperforming channels and regions to prioritize for improvement.

- Setting KPIs for adoption rate, click-through rate (CTR), and conversion uplift.

- Benchmarking bounce rate, revenue, and retention across platforms.

This groundwork provided a clear view of where the campaign needed to have the most impact and informed strategy alignment with product goals.

**Key Insights and Visualizations – Before the Campaign**

###  Balanced Platform Engagement, But Web Slightly Leads

![image](https://github.com/user-attachments/assets/fdd8f8ef-855a-4d00-9bce-2d5937987ee6)

Among the 500 total users, platform usage was fairly distributed:
- Web: 178 users  
- App: 170 users  
- Social: 152 users  

While the differences are modest, the Web platform had a slight lead in engagement. This suggests that pre-campaign targeting should not overly favor one platform, but Web may serve as a good testing ground for new features or messaging.

### Product Interest Was Inconsistent Across Pre-Campaign Months

![image](https://github.com/user-attachments/assets/b3f55f31-0182-41c7-a7c1-dae50f302780)

The line chart comparing average product views for **September**, **October**, and **November** reveals highly variable interest levels:

- **October** showed multiple sharp spikes, peaking around Day 11 and Day 16 with over 60 views, indicating periods of intense but short-lived user attention.
- **September** had the highest single-day spike, approaching 70 views around Day 21, but also featured frequent drop-offs.
- **November** demonstrated slightly more stable engagement but at generally lower levels compared to the prior months.

This inconsistency suggests that without campaign stimulation, product interest was largely erratic. These fluctuations informed the campaign’s objective to **stabilize user engagement and increase average daily product views** through consistent messaging and behavioral nudges.

## During the Campaign
![image](https://github.com/user-attachments/assets/42922c23-781b-4600-a55d-1705bf58985a)


https://github.com/user-attachments/assets/7f7e29ab-6be5-425b-af2e-651a19311c5b


During the active campaign period, real-time performance was monitored to assess traction and guide tactical adjustments. This phase included:

- Tracking key marketing KPIs: clicks, impressions, CTR, cost-per-click (CPC), email opens.

- Monitoring engagement across channels (App, Web, Email, Social).

- Visualizing trends in bounce rate, wishlist activity, and platform usage.

- Comparing performance by region and device type to refine targeting.

- Utilizing Power BI dashboards to enable near real-time insights for decision-makers.

The campaign was continuously optimized using these insights to maximize reach and ROI while staying agile in execution.

### Significant Variability in Ad Spend Allocation Across States

![image](https://github.com/user-attachments/assets/75693694-66de-46e4-ada0-5f9a65264ef0)

The "Daily Ad Spend by State" matrix highlights how ad budget was distributed across six U.S. states throughout the campaign:

- **Texas and Illinois consistently recorded the highest daily spend**, with values exceeding 1,700 units on multiple days (e.g., Days 5–9).
- **Colorado and Georgia received comparatively lower budgets**, though Georgia had notable spikes (Days 2 and 5).
- **Washington maintained a stable mid-to-high spend**, peaking on Days 5 and 10.

This variability suggests a strategic allocation of resources, likely aligned with regional performance forecasts or audience density. However, the fluctuating spend patterns also raise questions about **optimization consistency**, pointing to the need for more dynamic budget reallocation strategies in future campaigns based on live performance data.

### After the Campaign

![image](https://github.com/user-attachments/assets/cf30a49b-045a-4211-89de-625b485ec2a0)


https://github.com/user-attachments/assets/f9be0fe3-48f4-491d-8f84-0ffe9b68115e


Post-campaign analysis focused on evaluating outcomes against initial goals and drawing insights for future strategy. This included:

- Measuring increases in user acquisition, retention, and QuickSave goal completions.

- Analyzing revenue growth, conversion rates, and ROI by channel and region.

- Assessing performance variance between user segments and platforms.

- Identifying high-performing campaign components and underperforming areas.

- Summarizing findings into actionable recommendations for future campaigns.

The after-campaign evaluation not only demonstrated campaign impact but also generated strategic insights to guide long-term marketing planning.

### Revenue Peaked Midway Through the Post-Campaign Window

![image](https://github.com/user-attachments/assets/75683f67-8435-432b-a024-bd2bd3767911)

The line chart depicting "Revenue Generated Over Time" reveals a significant mid-period spike:

- Revenue grew sharply from **January 20 to January 24**, reaching a peak above 0.35K.
- After January 24, there was a noticeable drop, with revenue stabilizing at lower levels through to January 29.
- A final dip occurred on the last day, suggesting campaign momentum tapered off without sustained post-campaign activity.

This trend suggests that while the campaign successfully triggered a **strong short-term revenue lift**, it lacked mechanisms to **sustain user conversion or repeat behavior**. Future campaigns should consider incorporating post-campaign engagement strategies (e.g., email follow-ups, goal reminders) to maintain revenue consistency beyond peak days.

### Conversion Rate Showed Sharp Peaks with Poor Retention
![image](https://github.com/user-attachments/assets/133c1869-45e0-4ec8-9b65-ed1c14f5b2ba)

The "Conversion Rate Over Time" chart reveals an inconsistent pattern post-campaign:

- A strong rise in conversion rate occurred between **January 21 and January 23**, peaking at **0.26**.
- After the peak, conversion rates dropped sharply on **January 25 and 27**, falling below 0.14 before recovering slightly.
- The fluctuations suggest initial campaign excitement quickly faded, with limited sustained conversion momentum.

These results highlight a potential **gap in follow-through mechanisms**, such as nurture sequences, product reminders, or post-signup incentives. To improve future outcomes, campaigns should integrate strategies to **retain converted users and maintain momentum** beyond the campaign's active push.

## Tools and Techniques

This project was developed using Microsoft Power BI, leveraging its full suite of capabilities for data modeling, transformation, and visualization. The following tools and techniques were applied throughout the dashboard development process:

### Power BI Features Used

- **Power Query (M Language):**  
  Used for data extraction, transformation, and cleaning. This included:
  - Merging multiple data tables (user behavior, marketing spend, and revenue)
  - Filtering date ranges for each campaign phase
  - Creating custom columns for segmentation (e.g., user status, platform)

- **Data Modeling:**  
  A structured data model was created with clearly defined relationships to support cross-filtering and drill-throughs across campaign phases and user attributes.

- **DAX (Data Analysis Expressions):**  
  DAX was used extensively to create dynamic measures and KPIs such as:
  - Click-Through Rate (CTR)
  - Cost per Click (CPC)
  - Revenue by Channel
  - Conversion Rate and ROI
  - Average Product Views and Bounce Rate

- **Interactive Visualizations:**  
  Designed to be clean, intuitive, and segmented by campaign phase. Key features included:
  - Slicers for Location, Date, Channel, and Device
  - Time-series line charts and heatmaps
  - User segment breakdowns and funnel metrics

- **Custom Themes and Layouts:**  
  A consistent color scheme was applied for readability and branding, with each campaign phase having a distinct visual identity (dark theme for “Before,” light theme for “During,” neutral theme for “After”).

This combination of tools enabled a flexible, scalable, and insightful dashboard that supports both tactical campaign reviews and strategic planning.
## How to Use This Project

This Power BI dashboard is structured across three interactive pages—**Before Campaign**, **During Campaign**, and **After Campaign**; to provide a full view of the marketing campaign lifecycle.

### Navigating the Dashboard

- Use the **left-side navigation panel** to switch between campaign phases.(ctrl+click)
- Filters are available at the top of each page for:
  - **Location:** View metrics by U.S. state
  - **Date Range:** Analyze a specific campaign window
  - **Channel & Device Type:** Filter engagement by marketing source and device usage

### Interpreting Visuals

Each page includes:
- **Summary KPIs** at the top for high-level performance
- **Trend Charts** showing metrics over time (e.g., product views, revenue, conversion rate)
- **Segment Tables** and **Pie Charts** to break down user behavior by demographics, platform, and region
- **Heatmaps** (During Campaign only) to visualize ad spend distribution

Hover over any chart or data point to see exact values and breakdowns.
> **Note:** This project uses simulated data for illustrative purposes.

## Challenges

While developing the Marketing Campaign Analysis dashboard, several challenges were encountered:

### 1. Simulated Data Limitations
Since the project is based on mock data, certain real-world nuances—such as seasonality, user churn, or campaign fatigue, could not be fully modeled. This limited the ability to test long-term strategies or predict actual customer behavior with complete accuracy.

### 2. Balancing Simplicity with Depth
Designing a dashboard that is both beginner-friendly and analytically rich required iterative refinement. Key trade-offs included:
- Limiting the number of visuals per page to avoid clutter
- Simplifying DAX logic for performance without losing insight granularity

### 3. Interpreting Spiky Metrics
Metrics like conversion rate and bounce rate showed high volatility across days. This made it difficult to detect meaningful trends without aggregating or smoothing, which in turn risked hiding short-term spikes important to marketing teams.

Addressing these challenges involved prioritizing clarity, creating consistent user segments, and applying thoughtful visual design and filtering logic.

## Recommendations

Based on the analysis conducted across the Before, During, and After Campaign phases, the following recommendations are proposed to improve future marketing outcomes for the QuickSave feature:

### 1. Prioritize Mobile-First Engagement Strategies
Mobile users demonstrated the highest engagement and conversion activity throughout the campaign. Future campaigns should:
- Optimize landing pages and call-to-actions for mobile usability
- Focus ad budget and content delivery on mobile-heavy platforms
- Use SMS or in-app push notifications for behavioral nudges

### 2. Reactivate Dormant Users Through Personalized Incentives
The pre-campaign analysis revealed a large dormant user base, while Deal Seekers responded well to campaign prompts. Recommendations:
- Deploy targeted email campaigns offering goal-based rewards or matching deposits
- Use behavioral segmentation to personalize outreach frequency and messaging
- Highlight QuickSave’s flexibility and progress tracking to increase habit formation

### 3. Improve Post-Campaign Retention Mechanics
Although revenue and conversion rates spiked mid-campaign, both metrics dropped quickly afterward. To maintain momentum:
- Implement automated follow-ups (e.g., savings tips, goal reminders)
- Introduce limited-time rewards for users who continue saving post-campaign
- Monitor post-conversion behavior and retarget users showing signs of drop-off

By applying these recommendations, Ziwatech can strengthen user acquisition, deepen engagement, and build lasting savings behavior around the QuickSave feature.

## Conclusion

This Marketing Campaign Analysis dashboard provides a comprehensive view of QuickSave’s performance across the campaign lifecycle—Before, During, and After. By combining structured data modeling, clean visuals, and actionable metrics, it enables data-driven insights that support better marketing decisions.

Key takeaways from the project include:
- Understanding platform and user segment behavior before launch
- Monitoring and adjusting channel strategy in real time
- Evaluating the short- and long-term impact of campaign efforts

While based on simulated data, the approach and structure mirror real-world analysis workflows. This project can serve as a blueprint for future marketing initiatives, and it highlights the value of integrating Power BI into strategic campaign planning.

Future improvements may include:
- Incorporating real-time data feeds
- Adding predictive forecasting
- Layering in A/B test results to validate campaign variations


## Author & License

This project was created as part of a professional portfolio to demonstrate practical marketing analytics using Power BI.

- TikTok: [@wanja_analyst](https://www.tiktok.com/@wanja_analyst)  
- LinkedIn: [Susan Wanja Kariuki](https://www.linkedin.com/in/susan-wanja-1b63a6234/)  

© 2025 | Susan Wanja | Data Analytics Portfolio
